{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 段落とコードスニペットに分割処理する。",
   "id": "74cfe84053d385eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# data to Faiss-db\n",
    "import re\n",
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "import faiss\n",
    "\n",
    "# OpenAI APIキーを設定\n",
    "openai.api_key = 'your-api-key'\n",
    "\n",
    "# ファイルを読み込む\n",
    "with open('./text_generation.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# 段落ごとに分割\n",
    "paragraphs = re.split(r'\\n\\s*\\n', content)\n",
    "\n",
    "# コードスニペットを抽出する正規表現\n",
    "code_pattern = re.compile(r'```python(.*?)```', re.DOTALL)\n",
    "\n",
    "# extract & translate to code to prompt: xxxx, completions: yyyy\n",
    "paragraphs = [code_pattern.findall(paragraph) for paragraph in paragraphs]\n",
    "\n",
    "# 段落とコードスニペットをリストに格納\n",
    "chunks = []\n",
    "for paragraph in paragraphs:\n",
    "    code_matches = code_pattern.findall(paragraph)\n",
    "    if code_matches:\n",
    "        for code in code_matches:\n",
    "            chunks.append(code.strip())\n",
    "    else:\n",
    "        chunks.append(paragraph.strip())\n",
    "\n",
    "# 各チャンクをベクトル化する関数\n",
    "def get_embedding(text):\n",
    "    response = openai.Embedding.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    return response['data'][0]['embedding']\n",
    "\n",
    "# 各チャンクをベクトル化\n",
    "chunk_embeddings = []\n",
    "for chunk in chunks:\n",
    "    embedding = get_embedding(chunk)\n",
    "    chunk_embeddings.append(embedding)\n",
    "\n",
    "# ベクトルデータをデータフレームに変換\n",
    "df = pd.DataFrame(chunk_embeddings)\n",
    "df['chunk'] = chunks\n",
    "\n",
    "# ベクトルデータの正規化\n",
    "normalized_embeddings = normalize(df.drop(columns=['chunk']), axis=1)\n",
    "df_normalized = pd.DataFrame(normalized_embeddings)\n",
    "df_normalized['chunk'] = df['chunk']\n",
    "\n",
    "# Faissインデックスを作成\n",
    "d = df_normalized.shape[1] - 1  # ベクトルの次元数\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# ベクトルをFaissインデックスに追加\n",
    "index.add(np.array(df_normalized.drop(columns=['chunk'])))\n",
    "\n",
    "# Faissインデックスを保存\n",
    "faiss.write_index(index, 'chunk_embeddings.index')\n",
    "\n",
    "# 結果を確認\n",
    "print(df_normalized.head())\n",
    "\n",
    "# 検索機能の実装例\n",
    "def search_chunk(query, k=5):\n",
    "    query_embedding = get_embedding(query)\n",
    "    query_embedding = normalize([query_embedding], axis=1)\n",
    "    D, I = index.search(np.array(query_embedding).astype(np.float32), k)\n",
    "    return df_normalized.iloc[I[0]]['chunk'].tolist()\n",
    "\n",
    "# サンプル検索\n",
    "query = \"テキスト生成モデルの概要\"\n",
    "results = search_chunk(query)\n",
    "print(\"検索結果:\")\n",
    "for result in results:\n",
    "    print(result)\n"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Paragraph ,Code で抽出",
   "id": "8811bd2298ff1a04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 関数のテスト　<-------- コードの練習。部分の開発\n",
    "import re\n",
    "import glob\n",
    "from openai import OpenAI\n",
    "import pathlib\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "\n",
    "def list_txt_files():\n",
    "    # 現在のディレクトリ内の .txt ファイル名のリストを取得\n",
    "    txt_files = sorted(glob.glob('*.txt'))\n",
    "    return txt_files\n",
    "\n",
    "# ChatGPTのAPIを使用してpython-コードブロックに適切なタイトルを生成する関数\n",
    "def generate_subtitle(code_block, title):\n",
    "    prompt = f\"The title of this code is {title}. Here, a sentence describing the function in detail should be written as a subtitle, with a maximum length of 80 characters. The following is the code:\\n\\n{code_block}\"\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a professional python developer, a helpful assistant and good at chatgpt APIs.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        max_tokens=50,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.5\n",
    "    )\n",
    "    # title = response.choices[0].message.content\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def generate_question(paragraph, title):\n",
    "    prompt = f\"The task here is to read the attached explanatory text and create a question to which the explanatory text answers. Now, the attached explanatory text is part of an explanatory text whose title is {title}. Now, write only the question statement in 80 words or less. The following is the explanatory text that will serve as the answer.:\\n\\n{paragraph}\"\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a professional python developer, a helpful assistant and good at chatgpt APIs.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        max_tokens=50,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.5\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def split_into_paragraphs(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Pythonコードブロックを先に抽出し、それ以外の部分(pythonブロックを区切り文字として)を段落に分割\n",
    "    python_blocks = re.findall(r'```python.*?```', content, re.DOTALL)\n",
    "    non_python_parts = re.split(r'```python.*?```', content, flags=re.DOTALL)\n",
    "\n",
    "    paragraphs = []\n",
    "    \n",
    "    # 各非コード部分をさらに段落に分ける\n",
    "    for part in non_python_parts:\n",
    "        paragraphs.extend(part.strip().split('\\n\\n'))\n",
    "\n",
    "    return paragraphs, python_blocks\n",
    "\n",
    "file_lists = list_txt_files()\n",
    "paragraphs = list()\n",
    "python_blocks = list()\n",
    "\n",
    "python_code_to_dict = dict()\n",
    "paragraph_to_dict = dict()\n",
    "py_no = 1\n",
    "for file_path in file_lists:\n",
    "    file_name_without_extension = pathlib.Path(file_path).stem\n",
    "    # ここで文章から、パラグラフとpythonコードを取り出す。\n",
    "    paragraphs, python_blocks = split_into_paragraphs(file_path)\n",
    "    title = file_name_without_extension[3:]\n",
    "\n",
    "    for python_block in python_blocks:\n",
    "        subtitle = generate_subtitle(python_block, title)\n",
    "        python_title_subtitle_code = {\"no\": py_no, \"title\": title, \"subtitle\": subtitle, \"code\": python_block}\n",
    "        python_code_to_dict[py_no] = python_title_subtitle_code\n",
    "        py_no += 1\n",
    "            \n",
    "# Q/A => (jsonl - ex.) {\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "py_no = 1\n",
    "for file_path in file_lists:\n",
    "    file_name_without_extension = pathlib.Path(file_path).stem\n",
    "    # ここで文章から、パラグラフとpythonコードを取り出す。\n",
    "    paragraphs, python_blocks = split_into_paragraphs(file_path)\n",
    "    title = file_name_without_extension[3:]\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        question = generate_question(paragraph, title)\n",
    "        paragraph_dat = dict()\n",
    "        paragraph_dat[\"py_no\"] = py_no\n",
    "        paragraph_dat[\"title\"] = title\n",
    "        paragraph_dat[\"question\"] = question\n",
    "        paragraph_dat[\"paragraph\"] = paragraph\n",
    "        paragraph_to_dict[py_no] = paragraph_dat\n",
    "        py_no += 1\n",
    "        print(f'{py_no}_')\n",
    "        \n",
    "    # パラグラフをragデータ用とfine-tuning用にQAに取り出す。\n",
    "\n",
    "\n",
    "# 修正：json.dump() を使用して辞書をJSONファイルに書き込む\n",
    "with open('python_code_dict.json', 'w') as fp:\n",
    "    json.dump(python_code_to_dict, fp, indent=4)  # indent=4 でフォーマットを見やすく\n",
    "\n",
    "with open('paragraph_dict.json', 'w') as f:\n",
    "    json.dump(paragraph_to_dict, f, indent=4)\n"
   ],
   "id": "d3ed6d702ebb7200",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
