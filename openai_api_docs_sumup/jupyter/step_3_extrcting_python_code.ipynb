{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Extracting Python Code",
   "id": "b2ab0967872e573e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### 独自のopenai-python コード リポジトリを使用します。\n",
    "- 埋め込み、インデックス作成、クエリが可能な、Python ファイルからのファイル解析と\n",
    "- 関数の抽出のシンプルなバージョンを実装します。\n",
    "- https://github.com/openai/openai-python\n",
    "- *\n",
    "- 埋め込み、\n",
    "- インデックス作成、\n",
    "- クエリが可能な、Python ファイルからのファイル解析と\n",
    "- 関数の抽出のシンプルなバージョンを実装します。"
   ],
   "id": "c68b32412bfe2d83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Helper Function\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DEF_PREFIXES = ['def ', 'async def ']\n",
    "NEWLINE = '\\n'\n",
    "\n",
    "def get_function_name(code):\n",
    "    \"\"\"\n",
    "    Extract function name from a line beginning with 'def' or 'async def'.\n",
    "    \"\"\"\n",
    "    for prefix in DEF_PREFIXES:\n",
    "        if code.startswith(prefix):\n",
    "            return code[len(prefix): code.index('(')]\n",
    "\n",
    "\n",
    "def get_until_no_space(all_lines, i):\n",
    "    \"\"\"\n",
    "    Get all lines until a line outside the function definition is found.\n",
    "    \"\"\"\n",
    "    ret = [all_lines[i]]\n",
    "    for j in range(i + 1, len(all_lines)):\n",
    "        if len(all_lines[j]) == 0 or all_lines[j][0] in [' ', '\\t', ')']:\n",
    "            ret.append(all_lines[j])\n",
    "        else:\n",
    "            break\n",
    "    return NEWLINE.join(ret)\n",
    "\n",
    "\n",
    "def get_functions(filepath):\n",
    "    \"\"\"\n",
    "    Get all functions in a Python file.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r') as file:\n",
    "        all_lines = file.read().replace('\\r', NEWLINE).split(NEWLINE)\n",
    "        for i, l in enumerate(all_lines):\n",
    "            for prefix in DEF_PREFIXES:\n",
    "                if l.startswith(prefix):\n",
    "                    code = get_until_no_space(all_lines, i)\n",
    "                    function_name = get_function_name(code)\n",
    "                    yield {\n",
    "                        'code': code,\n",
    "                        'function_name': function_name,\n",
    "                        'filepath': filepath,\n",
    "                    }\n",
    "                    break\n",
    "\n",
    "\n",
    "def extract_functions_from_repo(code_root):\n",
    "    \"\"\"\n",
    "    Extract all .py functions from the repository.\n",
    "    \"\"\"\n",
    "    code_files = list(code_root.glob('**/*.py'))\n",
    "\n",
    "    num_files = len(code_files)\n",
    "    print(f'Total number of .py files: {num_files}')\n",
    "\n",
    "    if num_files == 0:\n",
    "        print('Verify openai-python repo exists and code_root is set correctly.')\n",
    "        return None\n",
    "\n",
    "    all_funcs = [\n",
    "        func\n",
    "        for code_file in code_files\n",
    "        for func in get_functions(str(code_file))\n",
    "    ]\n",
    "\n",
    "    num_funcs = len(all_funcs)\n",
    "    print(f'Total number of functions extracted: {num_funcs}')\n",
    "\n",
    "    return all_funcs\n",
    "\n",
    "# Load Data ----------------------------------------------------\n",
    "# Set user root directory to the 'openai-python' repository\n",
    "root_dir = Path.home()\n",
    "\n",
    "# Assumes the 'openai-python' repository exists in the user's root directory\n",
    "code_root = root_dir / 'openai-python'\n",
    "\n",
    "# Extract all functions from the repository\n",
    "all_funcs = extract_functions_from_repo(code_root)\n",
    "\n",
    "# text-embedding-3-smallベクトル埋め込み ---------------------------\n",
    "from utils.embeddings_utils import get_embedding\n",
    "\n",
    "df = pd.DataFrame(all_funcs)\n",
    "df['code_embedding'] = df['code'].apply(lambda x: get_embedding(x, model='text-embedding-3-small'))\n",
    "df['filepath'] = df['filepath'].map(lambda x: Path(x).relative_to(code_root))\n",
    "df.to_csv(\"data/code_search_openai-python.csv\", index=False)\n",
    "df.head()\n"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Python codeの抜き出し",
   "id": "f135c896a758b779"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T03:13:03.035279Z",
     "start_time": "2024-07-25T03:13:03.029581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# \n",
    "import re\n",
    "\n",
    "# ファイルを読み込む\n",
    "with open('./text/embedding.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# 正規表現を使って、```pythonと```の間にあるPythonコードを抽出する\n",
    "pattern = re.compile(r'```python(.*?)```', re.DOTALL)\n",
    "matches = pattern.findall(content)\n",
    "\n",
    "# 抽出されたコードを変数に代入する\n",
    "programs = []\n",
    "for match in matches:\n",
    "    programs.append(match.strip())\n",
    "\n",
    "# 結果を確認\n",
    "for i, program in enumerate(programs):\n",
    "    print(f\"Program {i+1}:--------------------\\n{program}\\n\")"
   ],
   "id": "3365f88ccb05fde9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program 1:--------------------\n",
      "from openai import OpenAI\n",
      "client = OpenAI()\n",
      "\n",
      "response = client.embeddings.create(\n",
      "    input=\"Your text string goes here\",\n",
      "    model=\"text-embedding-3-small\"\n",
      ")\n",
      "\n",
      "print(response.data[0].embedding)\n",
      "\n",
      "Program 2:--------------------\n",
      "from openai import OpenAI\n",
      "client = OpenAI()\n",
      "\n",
      "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
      "   text = text.replace(\"\\n\", \" \")\n",
      "   return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
      "\n",
      "df['ada_embedding'] = df.combined.apply(lambda x: get_embedding(x, model='text-embedding-3-small'))\n",
      "df.to_csv('output/embedded_1k_reviews.csv', index=False)\n",
      "\n",
      "Program 3:--------------------\n",
      "import pandas as pd\n",
      "\n",
      "df = pd.read_csv('output/embedded_1k_reviews.csv')\n",
      "df['ada_embedding'] = df.ada_embedding.apply(eval).apply(np.array)\n",
      "\n",
      "Program 4:--------------------\n",
      "from openai import OpenAI\n",
      "import numpy as np\n",
      "\n",
      "client = OpenAI()\n",
      "\n",
      "def normalize_l2(x):\n",
      "    x = np.array(x)\n",
      "    if x.ndim == 1:\n",
      "        norm = np.linalg.norm(x)\n",
      "        if norm == 0:\n",
      "            return x\n",
      "        return x / norm\n",
      "    else:\n",
      "        norm = np.linalg.norm(x, 2, axis=1, keepdims=True)\n",
      "        return np.where(norm == 0, x, x / norm)\n",
      "\n",
      "\n",
      "response = client.embeddings.create(\n",
      "    model=\"text-embedding-3-small\", input=\"Testing 123\", encoding_format=\"float\"\n",
      ")\n",
      "\n",
      "cut_dim = response.data[0].embedding[:256]\n",
      "norm_dim = normalize_l2(cut_dim)\n",
      "\n",
      "print(norm_dim)\n",
      "\n",
      "Program 5:--------------------\n",
      "query = f\"\"\"Use the below article on the 2022 Winter Olympics to answer the subsequent question. If the answer cannot be found, write \"I don't know.\"\n",
      "\n",
      "Article:\n",
      "\\\"\\\"\\\"\n",
      "{wikipedia_article_on_curling}\n",
      "\\\"\\\"\\\"\n",
      "\n",
      "Question: Which athletes won the gold medal in curling at the 2022 Winter Olympics?\"\"\"\n",
      "\n",
      "response = client.chat.completions.create(\n",
      "    messages=[\n",
      "        {'role': 'system', 'content': 'You answer questions about the 2022 Winter Olympics.'},\n",
      "        {'role': 'user', 'content': query},\n",
      "    ],\n",
      "    model=GPT_MODEL,\n",
      "    temperature=0,\n",
      ")\n",
      "\n",
      "print(response.choices[0].message.content)\n",
      "\n",
      "Program 6:--------------------\n",
      "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
      "\n",
      "def search_reviews(df, product_description, n=3, pprint=True):\n",
      "   embedding = get_embedding(product_description, model='text-embedding-3-small')\n",
      "   df['similarities'] = df.ada_embedding.apply(lambda x: cosine_similarity(x, embedding))\n",
      "   res = df.sort_values('similarities', ascending=False).head(n)\n",
      "   return res\n",
      "\n",
      "res = search_reviews(df, 'delicious beans', n=3)\n",
      "\n",
      "Program 7:--------------------\n",
      "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
      "\n",
      "df['code_embedding'] = df['code'].apply(lambda x: get_embedding(x, model='text-embedding-3-small'))\n",
      "\n",
      "def search_functions(df, code_query, n=3, pprint=True, n_lines=7):\n",
      "   embedding = get_embedding(code_query, model='text-embedding-3-small')\n",
      "   df['similarities'] = df.code_embedding.apply(lambda x: cosine_similarity(x, embedding))\n",
      "\n",
      "   res = df.sort_values('similarities', ascending=False).head(n)\n",
      "   return res\n",
      "res = search_functions(df, 'Completions API tests', n=3)\n",
      "\n",
      "Program 8:--------------------\n",
      "def recommendations_from_strings(\n",
      "   strings: List[str],\n",
      "   index_of_source_string: int,\n",
      "   model=\"text-embedding-3-small\",\n",
      ") -> List[int]:\n",
      "   \"\"\"Return nearest neighbors of a given string.\"\"\"\n",
      "\n",
      "   # get embeddings for all strings\n",
      "   embeddings = [embedding_from_string(string, model=model) for string in strings]\n",
      "\n",
      "   # get the embedding of the source string\n",
      "   query_embedding = embeddings[index_of_source_string]\n",
      "\n",
      "   # get distances between the source embedding and other embeddings (function from embeddings_utils.py)\n",
      "   distances = distances_from_embeddings(query_embedding, embeddings, distance_metric=\"cosine\")\n",
      "\n",
      "   # get indices of nearest neighbors (function from embeddings_utils.py)\n",
      "   indices_of_nearest_neighbors = indices_of_nearest_neighbors_from_distances(distances)\n",
      "   return indices_of_nearest_neighbors\n",
      "\n",
      "Program 9:--------------------\n",
      "import pandas as pd\n",
      "from sklearn.manifold import TSNE\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib\n",
      "\n",
      "df = pd.read_csv('output/embedded_1k_reviews.csv')\n",
      "matrix = df.ada_embedding.apply(eval).to_list()\n",
      "\n",
      "# Create a t-SNE model and transform the data\n",
      "tsne = TSNE(n_components=2, perplexity=15, random_state=42, init='random', learning_rate=200)\n",
      "vis_dims = tsne.fit_transform(matrix)\n",
      "\n",
      "colors = [\"red\", \"darkorange\", \"gold\", \"turquiose\", \"darkgreen\"]\n",
      "x = [x for x,y in vis_dims]\n",
      "y = [y for x,y in vis_dims]\n",
      "color_indices = df.Score.values - 1\n",
      "\n",
      "colormap = matplotlib.colors.ListedColormap(colors)\n",
      "plt.scatter(x, y, c=color_indices, cmap=colormap, alpha=0.3)\n",
      "plt.title(\"Amazon ratings visualized in language using t-SNE\")\n",
      "\n",
      "Program 10:--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    list(df.ada_embedding.values),\n",
      "    df.Score,\n",
      "    test_size = 0.2,\n",
      "    random_state=42\n",
      ")\n",
      "\n",
      "Program 11:--------------------\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "rfr = RandomForestRegressor(n_estimators=100)\n",
      "rfr.fit(X_train, y_train)\n",
      "preds = rfr.predict(X_test)\n",
      "\n",
      "Program 12:--------------------\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.metrics import classification_report, accuracy_score\n",
      "\n",
      "clf = RandomForestClassifier(n_estimators=100)\n",
      "clf.fit(X_train, y_train)\n",
      "preds = clf.predict(X_test)\n",
      "\n",
      "Program 13:--------------------\n",
      "from openai.embeddings_utils import cosine_similarity, get_embedding\n",
      "\n",
      "df= df[df.Score!=3]\n",
      "df['sentiment'] = df.Score.replace({1:'negative', 2:'negative', 4:'positive', 5:'positive'})\n",
      "\n",
      "labels = ['negative', 'positive']\n",
      "label_embeddings = [get_embedding(label, model=model) for label in labels]\n",
      "\n",
      "def label_score(review_embedding, label_embeddings):\n",
      "   return cosine_similarity(review_embedding, label_embeddings[1]) - cosine_similarity(review_embedding, label_embeddings[0])\n",
      "\n",
      "prediction = 'positive' if label_score('Sample Review', label_embeddings) > 0 else 'negative'\n",
      "\n",
      "Program 14:--------------------\n",
      "user_embeddings = df.groupby('UserId').ada_embedding.apply(np.mean)\n",
      "prod_embeddings = df.groupby('ProductId').ada_embedding.apply(np.mean)\n",
      "\n",
      "Program 15:--------------------\n",
      "import numpy as np\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "matrix = np.vstack(df.ada_embedding.values)\n",
      "n_clusters = 4\n",
      "\n",
      "kmeans = KMeans(n_clusters = n_clusters, init='k-means++', random_state=42)\n",
      "kmeans.fit(matrix)\n",
      "df['Cluster'] = kmeans.labels_\n",
      "\n",
      "Program 16:--------------------\n",
      "import tiktoken\n",
      "\n",
      "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
      "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
      "    encoding = tiktoken.get_encoding(encoding_name)\n",
      "    num_tokens = len(encoding.encode(string))\n",
      "    return num_tokens\n",
      "\n",
      "num_tokens_from_string(\"tiktoken is great!\", \"cl100k_base\")\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# AST\n",
    "import re\n",
    "import ast\n",
    "\n",
    "# ファイルを読み込む\n",
    "with open('./text/embedding.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# 正規表現を使って、```pythonと```の間にあるPythonコードを抽出する\n",
    "pattern = re.compile(r'```python(.*?)```', re.DOTALL)\n",
    "matches = pattern.findall(content)\n",
    "\n",
    "# ASTを使ってチャンクを抽出する関数\n",
    "def extract_chunks(code):\n",
    "    chunks = []\n",
    "    tree = ast.parse(code)\n",
    "    \n",
    "    current_chunk = []\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = []\n",
    "            current_chunk.append(node)\n",
    "        elif isinstance(node, ast.Call):\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = []\n",
    "            current_chunk.append(node)\n",
    "        elif isinstance(node, ast.stmt):\n",
    "            current_chunk.append(node)\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# コードをチャンク化する\n",
    "all_chunks = []\n",
    "for match in matches:\n",
    "    code = match.strip()\n",
    "    chunks = extract_chunks(code)\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "# チャンクを元のコードに戻す関数\n",
    "def unparse_chunk(chunk):\n",
    "    return \"\\n\".join(ast.unparse(node) for node in chunk)\n",
    "\n",
    "# 各チャンクを表示\n",
    "for i, chunk in enumerate(all_chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{unparse_chunk(chunk)}\\n\")\n",
    "\n",
    "# ここで、ベクトル化のためのコードを追加する\n",
    "# 例: 各チャンクを文字列として保存し、ベクトル化する準備を行う\n",
    "chunks_as_strings = [unparse_chunk(chunk) for chunk in all_chunks]\n",
    "\n",
    "# ベクトル化の例（ここでは簡単に文字列の長さをベクトルとして使用）\n",
    "vectors = [len(chunk) for chunk in chunks_as_strings]\n",
    "\n",
    "# 結果を確認\n",
    "for i, vector in enumerate(vectors):\n",
    "    print(f\"Vector {i+1}: {vector}\")\n"
   ],
   "id": "a55b2acd9c36e21c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Final python code\n",
    "import re\n",
    "import ast\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# OpenAI APIキーを設定\n",
    "# openai.api_key = 'your-api-key'\n",
    "\n",
    "# ファイルを読み込む\n",
    "with open('./text/embedding.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# 正規表現を使って、```pythonと```の間にあるPythonコードを抽出する\n",
    "pattern = re.compile(r'```python(.*?)```', re.DOTALL)\n",
    "matches = pattern.findall(content)\n",
    "\n",
    "# 抽出されたコードを変数に代入する\n",
    "programs = []\n",
    "for match in matches:\n",
    "    programs.append(match.strip())\n",
    "\n",
    "# ASTを使って関数レベルでチャンクを抽出する関数\n",
    "def extract_functions(code):\n",
    "    functions = []\n",
    "    tree = ast.parse(code)\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.FunctionDef):\n",
    "            functions.append(ast.unparse(node))\n",
    "    return functions\n",
    "\n",
    "# 関数レベルでのチャンク化\n",
    "all_functions = []\n",
    "for program in programs:\n",
    "    functions = extract_functions(program)\n",
    "    all_functions.extend(functions)\n",
    "\n",
    "# 各関数をベクトル化する関数\n",
    "def get_embedding(text):\n",
    "    client = OpenAI()\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# 各関数をベクトル化\n",
    "function_embeddings = []\n",
    "for func in all_functions:\n",
    "    embedding = get_embedding(func)\n",
    "    function_embeddings.append(embedding)\n",
    "\n",
    "# ベクトルデータをデータフレームに変換\n",
    "df = pd.DataFrame(function_embeddings)\n",
    "df['function'] = all_functions\n",
    "\n",
    "# ベクトルデータの正規化\n",
    "normalized_embeddings = normalize(df.drop(columns=['function']), axis=1)\n",
    "df_normalized = pd.DataFrame(normalized_embeddings)\n",
    "df_normalized['function'] = df['function']\n",
    "\n",
    "# Vector Storeへの登録（ここではFaissを例に使用）\n",
    "import faiss\n",
    "\n",
    "# Faissインデックスを作成\n",
    "d = df_normalized.shape[1] - 1  # ベクトルの次元数\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# ベクトルをFaissインデックスに追加\n",
    "index.add(np.array(df_normalized.drop(columns=['function'])))\n",
    "\n",
    "# Faissインデックスを保存\n",
    "faiss.write_index(index, 'function_embeddings.index')\n",
    "\n",
    "# 結果を確認\n",
    "print(df_normalized.head())\n",
    "\n",
    "# 検索機能の実装例\n",
    "def search_function(query, k=5):\n",
    "    query_embedding = get_embedding(query)\n",
    "    query_embedding = normalize([query_embedding], axis=1)\n",
    "    D, I = index.search(np.array(query_embedding).astype(np.float32), k)\n",
    "    return df_normalized.iloc[I[0]]['function'].tolist()\n",
    "\n",
    "# サンプル検索\n",
    "query = \"データを読み込む関数\"\n",
    "results = search_function(query)\n",
    "print(\"検索結果:\")\n",
    "for result in results:\n",
    "    print(result)\n"
   ],
   "id": "994be760aac53f99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Final doc faiss \n",
    "import re\n",
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "import faiss\n",
    "\n",
    "# OpenAI APIキーを設定\n",
    "# openai.api_key = 'your-api-key'\n",
    "\n",
    "# ファイルを読み込む\n",
    "with open('./text/text_generation.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# 段落ごとに分割\n",
    "paragraphs = re.split(r'\\n\\s*\\n', content)\n",
    "\n",
    "# コードスニペットを抽出する正規表現\n",
    "code_pattern = re.compile(r'```python(.*?)```', re.DOTALL)\n",
    "\n",
    "# 段落とコードスニペットをリストに格納\n",
    "chunks = []\n",
    "for paragraph in paragraphs:\n",
    "    code_matches = code_pattern.findall(paragraph)\n",
    "    if code_matches:\n",
    "        for code in code_matches:\n",
    "            chunks.append(code.strip())\n",
    "    else:\n",
    "        chunks.append(paragraph.strip())\n",
    "\n",
    "# 各チャンクをベクトル化する関数\n",
    "def get_embedding(text):\n",
    "    client = OpenAI()\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# 各チャンクをベクトル化\n",
    "chunk_embeddings = []\n",
    "for chunk in chunks:\n",
    "    embedding = get_embedding(chunk)\n",
    "    chunk_embeddings.append(embedding)\n",
    "\n",
    "# ベクトルデータをデータフレームに変換\n",
    "df = pd.DataFrame(chunk_embeddings)\n",
    "df['chunk'] = chunks\n",
    "\n",
    "# ベクトルデータの正規化\n",
    "normalized_embeddings = normalize(df.drop(columns=['chunk']), axis=1)\n",
    "df_normalized = pd.DataFrame(normalized_embeddings)\n",
    "df_normalized['chunk'] = df['chunk']\n",
    "\n",
    "# Faissインデックスを作成\n",
    "d = df_normalized.shape[1] - 1  # ベクトルの次元数\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# ベクトルをFaissインデックスに追加\n",
    "index.add(np.array(df_normalized.drop(columns=['chunk'])))\n",
    "\n",
    "# Faissインデックスを保存\n",
    "faiss.write_index(index, 'chunk_embeddings.index')\n",
    "\n",
    "# 結果を確認\n",
    "print(df_normalized.head())\n",
    "\n",
    "# 検索機能の実装例\n",
    "def search_chunk(query, k=5):\n",
    "    query_embedding = get_embedding(query)\n",
    "    query_embedding = normalize([query_embedding], axis=1)\n",
    "    D, I = index.search(np.array(query_embedding).astype(np.float32), k)\n",
    "    return df_normalized.iloc[I[0]]['chunk'].tolist()\n",
    "\n",
    "# サンプル検索\n",
    "query = \"テキスト生成モデルの概要\"\n",
    "results = search_chunk(query)\n",
    "print(\"検索結果:\")\n",
    "for result in results:\n",
    "    print(result)\n"
   ],
   "id": "e4d594c35f60b22a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Chorome DB\n",
    "# pip install chromedb\n",
    "# chrome_db_api_key = 'your-chromedb-api-key'\n",
    "# chrome_db_url = 'your-chromedb-url'\n",
    "# Final doc ChromeDB \n",
    "import re\n",
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# OpenAI APIキーを設定\n",
    "# openai.api_key = 'your-api-key'\n",
    "\n",
    "# ファイルを読み込む\n",
    "with open('./text/text_generation.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# 段落ごとに分割\n",
    "paragraphs = re.split(r'\\n\\s*\\n', content)\n",
    "\n",
    "# コードスニペットを抽出する正規表現\n",
    "code_pattern = re.compile(r'```python(.*?)```', re.DOTALL)\n",
    "\n",
    "# 段落とコードスニペットをリストに格納\n",
    "chunks = []\n",
    "for paragraph in paragraphs:\n",
    "    code_matches = code_pattern.findall(paragraph)\n",
    "    if code_matches:\n",
    "        for code in code_matches:\n",
    "            chunks.append(code.strip())\n",
    "    else:\n",
    "        chunks.append(paragraph.strip())\n",
    "\n",
    "# OpenAI Embedding 関数を設定\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=\"your-openai-api-key\",\n",
    "    model_name=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "# ChromaDBクライアントを作成\n",
    "client = chromadb.Client()\n",
    "\n",
    "# コレクションを作成\n",
    "collection = client.create_collection(name=\"text_generation_chunks\", embedding_function=openai_ef)\n",
    "\n",
    "# チャンクをコレクションに追加\n",
    "collection.add(\n",
    "    documents=chunks,\n",
    "    ids=[f\"chunk_{i}\" for i in range(len(chunks))]\n",
    ")\n",
    "\n",
    "# 検索機能の実装例\n",
    "def search_chunk(query, k=5):\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=k\n",
    "    )\n",
    "    return results['documents'][0]\n",
    "\n",
    "# サンプル検索\n",
    "query = \"テキスト生成モデルの概要\"\n",
    "results = search_chunk(query)\n",
    "print(\"検索結果:\")\n",
    "for result in results:\n",
    "    print(result)"
   ],
   "id": "a0f839f7d3cc56d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "eedb527c2cbfb244"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Milvau の場合\n",
    "# Final doc Milvus\n",
    "- docker run -d --name milvus-standalone -p 19530:19530 -p 9091:9091 milvusdb/milvus:latest\n"
   ],
   "id": "b60108012aca4402"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# case Milvus\n",
    "# !pip install pymilvus\n",
    "\"\"\"docker run -d --name milvus-standalone -p 19530:19530 -p 9091:9091 milvusdb/milvus:latest\"\"\"\n",
    "import re\n",
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection\n",
    "\n",
    "# Milvusサーバーに接続\n",
    "connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
    "\n",
    "# Milvusの設定\n",
    "collection_name = \"text_chunks\"\n",
    "\n",
    "# コレクションが既に存在する場合は削除\n",
    "if Collection.exists(collection_name):\n",
    "    collection = Collection(collection_name)\n",
    "    collection.drop()\n",
    "\n",
    "# コレクションのスキーマを定義\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=768),  # ベクトルの次元数\n",
    "    FieldSchema(name=\"chunk\", dtype=DataType.VARCHAR, max_length=65535)\n",
    "]\n",
    "schema = CollectionSchema(fields, description=\"Collection of text chunks and their embeddings\")\n",
    "collection = Collection(name=collection_name, schema=schema)\n",
    "\n",
    "# OpenAI APIキーを設定\n",
    "# openai.api_key = 'your-api-key'\n",
    "\n",
    "# ファイルを読み込む\n",
    "with open('./text/text_generation.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# 段落ごとに分割\n",
    "paragraphs = re.split(r'\\n\\s*\\n', content)\n",
    "\n",
    "# コードスニペットを抽出する正規表現\n",
    "code_pattern = re.compile(r'```python(.*?)```', re.DOTALL)\n",
    "\n",
    "# 段落とコードスニペットをリストに格納\n",
    "chunks = []\n",
    "for paragraph in paragraphs:\n",
    "    code_matches = code_pattern.findall(paragraph)\n",
    "    if code_matches:\n",
    "        for code in code_matches:\n",
    "            chunks.append(code.strip())\n",
    "    else:\n",
    "        chunks.append(paragraph.strip())\n",
    "\n",
    "# 各チャンクをベクトル化する関数\n",
    "def get_embedding(text):\n",
    "    client = OpenAI()\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# 各チャンクをベクトル化\n",
    "chunk_embeddings = []\n",
    "for chunk in chunks:\n",
    "    embedding = get_embedding(chunk)\n",
    "    chunk_embeddings.append(embedding)\n",
    "\n",
    "# ベクトルデータをデータフレームに変換\n",
    "df = pd.DataFrame(chunk_embeddings)\n",
    "df['chunk'] = chunks\n",
    "\n",
    "# ベクトルデータの正規化\n",
    "normalized_embeddings = normalize(df.drop(columns=['chunk']), axis=1)\n",
    "df_normalized = pd.DataFrame(normalized_embeddings)\n",
    "df_normalized['chunk'] = df['chunk']\n",
    "\n",
    "# Milvusにベクトルデータを追加\n",
    "insert_data = [\n",
    "    df.index.tolist(),  # IDとしてインデックスを使用\n",
    "    normalized_embeddings.tolist(),\n",
    "    df['chunk'].tolist()\n",
    "]\n",
    "collection.insert(insert_data)\n",
    "\n",
    "# インデックスを作成\n",
    "collection.create_index(field_name=\"embedding\", index_params={\"index_type\": \"IVF_FLAT\", \"params\": {\"nlist\": 128}})\n",
    "\n",
    "# コレクションのロード\n",
    "collection.load()\n",
    "\n",
    "# 結果を確認\n",
    "print(df_normalized.head())\n",
    "\n",
    "# 検索機能の実装例\n",
    "def search_chunk(query, k=5):\n",
    "    query_embedding = get_embedding(query)\n",
    "    query_embedding = normalize([query_embedding], axis=1).tolist()[0]\n",
    "    search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "    results = collection.search([query_embedding], \"embedding\", search_params, limit=k)\n",
    "    return [result.entity.get(\"chunk\") for result in results[0]]\n",
    "\n",
    "# サンプル検索\n",
    "query = \"テキスト生成モデルの概要\"\n",
    "results = search_chunk(query)\n",
    "print(\"検索結果:\")\n",
    "for result in results:\n",
    "    print(result)"
   ],
   "id": "abd6a293693d1263"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
